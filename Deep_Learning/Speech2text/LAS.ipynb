{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_hw4p2_lockdrop_wt_tying",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCfV_SDTNQrx"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa1d2AsRNlu4"
      },
      "source": [
        "# !pip install kaggle\n",
        "# !mkdir .kaggle\n",
        "# import json\n",
        "# token = {\"username\":\"saiprahladhp\",\"key\":\"f253b9da706f456d8092de8a09590c5b\"}\n",
        "# with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "#     json.dump(token, file)\n",
        "# !chmod 600 /content/.kaggle/kaggle.json\n",
        "# !cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "# !kaggle config path -p /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqvHPw9dN7EG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJY299V6OWnq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjmYdXzIeY9z"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqvhRe8iEk3q"
      },
      "source": [
        "!pip install python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azuRYnzzOgFP"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from torch.nn.utils.rnn import *\n",
        "from statistics import mean \n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.utils as utils\n",
        "import operator\n",
        "import Levenshtein\n",
        "import seaborn as sns\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M87hshQdek-U"
      },
      "source": [
        "speech_train = np.load('train.npy', allow_pickle=True, encoding='bytes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XETPWbMlg3j4"
      },
      "source": [
        "speech_valid = np.load('dev.npy', allow_pickle=True, encoding='bytes')\n",
        "speech_test = np.load('test.npy', allow_pickle=True, encoding='bytes')\n",
        "\n",
        "transcript_train = np.load('./train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "transcript_valid = np.load('./dev_transcripts.npy', allow_pickle=True,encoding='bytes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln-XqwAsj5OU"
      },
      "source": [
        "LETTER_LIST = ['<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
        "               'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-', \"'\", '.', '_', '+', ' ','<sos>','<eos>']    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aqRzGhln8VH"
      },
      "source": [
        "**Helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIPW_LQTmQDo"
      },
      "source": [
        "def transform_letter_to_index(transcript, letter_list,letter2index):\n",
        "    '''\n",
        "    :param transcript :(N, ) Transcripts are the text input\n",
        "    :param letter_list: Letter list defined above\n",
        "    :return letter_to_index_list: Returns a list for all the transcript sentence to index\n",
        "    '''\n",
        "    letter_to_index_list = []\n",
        "    for line in transcript:\n",
        "      letter_indices = [letter2index['<sos>']]\n",
        "      \n",
        "      for idx,word in enumerate(line):\n",
        "        keys = list(word.decode()) \n",
        "\n",
        "        vals = operator.itemgetter(*keys)(letter2index)\n",
        "        \n",
        "        if isinstance(vals,tuple):\n",
        "          vals = list(vals)\n",
        "        else:\n",
        "          vals = [vals]\n",
        "        \n",
        "        letter_indices += vals\n",
        "\n",
        "        if idx < len(line)-1:\n",
        "\n",
        "          letter_indices.append(letter2index[' '])\n",
        "\n",
        "      letter_indices.append(letter2index['<eos>'])\n",
        "      letter_to_index_list.append(letter_indices)\n",
        "\n",
        "\n",
        "    return letter_to_index_list\n",
        "\n",
        "'''\n",
        "Optional, create dictionaries for letter2index and index2letter transformations\n",
        "'''\n",
        "def create_dictionaries(letter_list):\n",
        "    indices = [*range(len(letter_list))]\n",
        "\n",
        "    letter2index = dict(zip(letter_list,indices))\n",
        "    index2letter = dict(zip(indices,letter_list))\n",
        "    return letter2index, index2letter\n",
        "\n",
        "def transform_index_to_letter(index,letter2index,index2letter):\n",
        "  \n",
        "  if(torch.is_tensor(index)):\n",
        "    index = index.numpy()\n",
        "\n",
        "  index_to_letter_list = []\n",
        "  breaks = [letter2index['<eos>'], letter2index['<pad>']]\n",
        "  for idx in index:\n",
        "      pred = \"\"\n",
        "      for i in idx:\n",
        "          #characterwise looping of the sentence\n",
        "          if i in breaks:\n",
        "              break\n",
        "          elif i == letter2index['<sos>']:\n",
        "            pred+=\"\"\n",
        "          else:\n",
        "              pred += index2letter[i]\n",
        "      index_to_letter_list.append(pred)\n",
        "  return index_to_letter_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxI4_kk0AYob"
      },
      "source": [
        "**Dataset Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpD8DuG-AXcC"
      },
      "source": [
        "class Speech2TextDataset(Dataset):\n",
        "    '''\n",
        "    Dataset class for the speech to text data, this may need some tweaking in the\n",
        "    getitem method as your implementation in the collate function may be different from\n",
        "    ours. \n",
        "    '''\n",
        "    def __init__(self, speech, text=None, isTrain=True):\n",
        "        self.speech = speech\n",
        "        self.isTrain = isTrain\n",
        "        if (text is not None):\n",
        "            self.text = text\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.speech.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if (self.isTrain == True):\n",
        "            return torch.tensor(self.speech[index].astype(np.float32)), torch.tensor(self.text[index])\n",
        "        else:\n",
        "            return torch.tensor(self.speech[index].astype(np.float32))\n",
        "\n",
        "\n",
        "def collate_train(batch_data):\n",
        "    ### Return the padded speech and text data, and the length of utterance and transcript ###\n",
        "    \n",
        "    speech_pad = pad_sequence([b[0] for b in batch_data],batch_first= True)\n",
        "\n",
        "    speech_lens = torch.tensor([len(b[0]) for b in batch_data])\n",
        "\n",
        "    text_pad = pad_sequence([b[1][1:] for b in batch_data],batch_first= True)\n",
        "\n",
        "    text_lens = torch.tensor([len(b[1])-1 for b in batch_data])\n",
        "\n",
        "    return speech_pad,speech_lens,text_pad,text_lens\n",
        "\n",
        "\n",
        "def collate_test(batch_data):\n",
        "    ### Return padded speech and length of utterance ###\n",
        "    speech_pad = pad_sequence([b for b in batch_data],batch_first=True)\n",
        "\n",
        "    speech_lens = torch.tensor([len(b) for b in batch_data])\n",
        "\n",
        "    return speech_pad,speech_lens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhfpoiXOnWDG"
      },
      "source": [
        "letter2index, index2letter = create_dictionaries(LETTER_LIST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lsO6_rT36rB"
      },
      "source": [
        "**Transformation of train and validation transcripts to indices**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWqJpQmoKK8x"
      },
      "source": [
        "train_letter_to_index = transform_letter_to_index(transcript_train,LETTER_LIST,letter2index)\n",
        "valid_letter_to_index = transform_letter_to_index(transcript_valid,LETTER_LIST,letter2index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OePEEgZhCS3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb483db-4ef3-4f16-e163-41d82012797b"
      },
      "source": [
        "print(transform_index_to_letter(train_letter_to_index,letter2index,index2letter)[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "which was pretty good evidence frank thought that the wounded boy must take considerable interest in the discussion why who else would try to turn on mister darrel that way and burn his shanties down just when winter is setting in asked bluff\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giIWgJ-Dzuzu"
      },
      "source": [
        "train_data = Speech2TextDataset(speech_train,train_letter_to_index)\n",
        "valid_data = Speech2TextDataset(speech_valid,valid_letter_to_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cIY3MMzfhAo"
      },
      "source": [
        "test_data = Speech2TextDataset(speech_test,isTrain=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI8c71dF1zLF"
      },
      "source": [
        "train_loader = DataLoader(train_data,batch_size=64,shuffle=True,collate_fn=collate_train)\n",
        "valid_loader = DataLoader(valid_data,batch_size=64,shuffle=False,collate_fn=collate_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgBj7hM3f-bK"
      },
      "source": [
        "test_loader = DataLoader(test_data,batch_size=64,shuffle=False,collate_fn=collate_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF6NT5gW3jBr"
      },
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G7dnq9htgm1"
      },
      "source": [
        "**Model Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1kHdn0dmKUW"
      },
      "source": [
        "1. **Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZsSz1NRLF8F"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using key, value and query from Encoder and decoder.\n",
        "    Below are the set of operations you need to perform for computing attention:\n",
        "        energy = bmm(key, query)\n",
        "        attention = softmax(energy)\n",
        "        context = bmm(attention, value)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def forward(self, query, key, value, lens):\n",
        "        '''\n",
        "        :param query :(batch_size, hidden_size) Query is the output of LSTMCell from Decoder\n",
        "        :param keys: (batch_size, max_len, encoder_size) Key Projection from Encoder\n",
        "        :param values: (batch_size, max_len, encoder_size) Value Projection from Encoder\n",
        "        :return context: (batch_size, encoder_size) Attended Context\n",
        "        :return attention_mask: (batch_size, max_len) Attention mask that can be plotted \n",
        "        '''\n",
        "        key = torch.transpose(key,0,1)\n",
        "        value = torch.transpose(value,0,1)\n",
        "\n",
        "        energy = torch.bmm(key,query.unsqueeze(2)).squeeze(2)\n",
        "        \n",
        "        mask = (torch.arange(key.size(1)).unsqueeze(0) >= lens.unsqueeze(1)).to(DEVICE)\n",
        "        \n",
        "\n",
        "        energy.masked_fill_(mask, -1e9)\n",
        "\n",
        "        attention_mask = F.softmax(energy, dim=1)\n",
        "\n",
        "        context = torch.bmm(attention_mask.unsqueeze(1),value).squeeze(1)\n",
        "\n",
        "        return context,attention_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmsDZQmV42pz"
      },
      "source": [
        "#DUMMY Calculation for creating binary mask\n",
        "\n",
        "# np.random.seed(1)\n",
        "# X = np.random.random((4,6)).round(1) * 2 + 3\n",
        "# X = torch.from_numpy(X)\n",
        "# X_len = torch.LongTensor([4, 1, 6, 3])  # length of each sequence\n",
        "# print(X)\n",
        "# max_len = X.shape[1]\n",
        "# mask = torch.arange(max_len)[None,:] >= X_len[:,None]\n",
        "# X[mask] = float('-inf')\n",
        "# print(X)\n",
        "# print(F.softmax(X, dim=1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYA22E0TiOm7"
      },
      "source": [
        "**Locked Dropout**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbFDbhcziSMR"
      },
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, dropout=0.5):\n",
        "        if not self.training or not dropout:\n",
        "            return x\n",
        "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
        "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT9rS2PcmOq7"
      },
      "source": [
        "**2. Pyramidal BiLSTM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lIcDVTuBYjJ"
      },
      "source": [
        "class pBLSTM(nn.Module):\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    The length of utterance (speech input) can be hundereds to thousands of frames long.\n",
        "    The Paper reports that a direct LSTM implementation as Encoder resulted in slow convergence,\n",
        "    and inferior results even after extensive training.\n",
        "    The major reason is inability of AttendAndSpell operation to extract relevant information\n",
        "    from a large number of input steps.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.blstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True, batch_first = True)\n",
        "       \n",
        "        self.dropout = LockedDropout()\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :param x :(N, T) input to the pBLSTM\n",
        "        :return output: (N, T, H) encoded sequence from pyramidal Bi-LSTM \n",
        "        '''\n",
        "        out, out_lens = pad_packed_sequence(x, batch_first= True)\n",
        "\n",
        "        #Implementing locked dropout\n",
        "        out = self.dropout(out,0.2)    \n",
        "\n",
        "        out_lens = out_lens.to(DEVICE)\n",
        "        #Dealing with odd dimensions:\n",
        "\n",
        "        batch,Length,dim = out.shape\n",
        "\n",
        "        if Length%2==1:\n",
        "          out_cropped = Length-1\n",
        "        else:\n",
        "          out_cropped = Length\n",
        "        out = out[:,:out_cropped,:]\n",
        "\n",
        "        out_reshaped = out.reshape(batch,Length//2,dim*2)\n",
        "\n",
        "        out_lens = out_lens//2\n",
        "\n",
        "        out_lens = out_lens.cpu()\n",
        "\n",
        "        packed_x = pack_padded_sequence(out_reshaped,lengths=out_lens,batch_first= True, enforce_sorted= False)\n",
        "\n",
        "        pack_out = self.blstm(packed_x)[0]\n",
        "\n",
        "        return pack_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew0owuorFPgT"
      },
      "source": [
        "**Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0M_x5YsFGQS"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Encoder takes the utterances as inputs and returns the key and value.\n",
        "    Key and value are nothing but simple projections of the output from pBLSTM network.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim, value_size=128,key_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=1, bidirectional=True)\n",
        "        \n",
        "        ### Add code to define the blocks of pBLSTMs! ###\n",
        "        list_pBLSTM = [pBLSTM(hidden_dim*4,hidden_dim)]*3\n",
        "        self.pBLSTM = nn.Sequential(*list_pBLSTM)\n",
        "        \n",
        "        self.key_network = nn.Linear(hidden_dim*2, value_size)\n",
        "        self.value_network = nn.Linear(hidden_dim*2, key_size)\n",
        "\n",
        "    def forward(self, x, lens):\n",
        "        lens = lens.cpu()\n",
        "        rnn_inp = utils.rnn.pack_padded_sequence(x, lengths=lens, batch_first=True, enforce_sorted=False)\n",
        "        outputs, _ = self.lstm(rnn_inp)\n",
        "\n",
        "        ### Use the outputs and pass it through the pBLSTM blocks! ###\n",
        "        outputs = self.pBLSTM(outputs)\n",
        "        linear_input, lens = utils.rnn.pad_packed_sequence(outputs)\n",
        "        keys = self.key_network(linear_input)\n",
        "        value = self.value_network(linear_input)\n",
        "\n",
        "        return keys, value, lens\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH6-EPA-jpTz"
      },
      "source": [
        "**Teacher forcing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md1nfx_vjorJ"
      },
      "source": [
        "def teacher_forcing(prediction,iter,batch_size,embedding_layer,embeddings,percentage,isTrain):\n",
        "\n",
        "  #Training\n",
        "  if (isTrain):\n",
        "\n",
        "    gen = np.random.random(1)[0]\n",
        "\n",
        "    if gen < (percentage/100):\n",
        "      if iter == 0:\n",
        "        sos = torch.ones(batch_size,dtype=torch.long)\n",
        "        sos = sos*(letter2index['<sos>'])\n",
        "        sos = sos.to(DEVICE)        \n",
        "        char_embed = embedding_layer(sos)\n",
        "\n",
        "      else:\n",
        "        char_embed = embeddings[:,iter-1,:]\n",
        "    else:\n",
        "      \n",
        "      char_embed = embedding_layer(prediction.argmax(dim = -1))\n",
        "  \n",
        "  #Testing\n",
        "  else:\n",
        "    if iter == 0:\n",
        "        sos = torch.ones(batch_size,dtype=torch.long)\n",
        "        sos = sos*(letter2index['<sos>'])\n",
        "        sos = sos.to(DEVICE)\n",
        "        char_embed = embedding_layer(sos)\n",
        "\n",
        "    else:\n",
        "        char_embed = embedding_layer(prediction.argmax(dim = -1))\n",
        "\n",
        "\n",
        "  return char_embed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5OKRatkOzDd"
      },
      "source": [
        "**Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT-HjigbNv5H"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step, \n",
        "    thus we use LSTMCell instead of LSLTM here.\n",
        "    The output from the second LSTMCell can be used as query here for attention module.\n",
        "    In place of value that we get from the attention, this can be replace by context we get from the attention.\n",
        "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
        "    '''\n",
        "    def __init__(self, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=False):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=0)\n",
        "        self.lstm1 = nn.LSTMCell(input_size=hidden_dim + value_size, hidden_size=hidden_dim)\n",
        "        self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)\n",
        "        \n",
        "        self.isAttended = isAttended\n",
        "        if (isAttended == True):\n",
        "            self.attention = Attention()\n",
        "\n",
        "        self.character_prob = nn.Linear(key_size + value_size, vocab_size)\n",
        "\n",
        "        #weight tying\n",
        "        self.character_prob.weight = self.embedding.weight\n",
        "\n",
        "    #Changing arguments to include batch_idx\n",
        "    def forward(self, key, values, lens, batch_idx, text=None, isTrain=True, teacher_forcing_percent = 90):\n",
        "        '''\n",
        "        :param key :(T, N, key_size) Output of the Encoder Key projection layer\n",
        "        :param values: (T, N, value_size) Output of the Encoder Value projection layer\n",
        "        :param text: (N, text_len) Batch input of text with text_length\n",
        "        :param isTrain: Train or eval mode\n",
        "        :return predictions: Returns the character perdiction probability \n",
        "        '''\n",
        "        batch_size = key.shape[1]\n",
        "\n",
        "        if (isTrain == True):\n",
        "            max_len =  text.shape[1]\n",
        "            embeddings = self.embedding(text)\n",
        "        else:\n",
        "            max_len = 600\n",
        "\n",
        "        predictions = []\n",
        "        hidden_states = [None, None]\n",
        "        prediction = torch.zeros(batch_size,1).to(DEVICE)#(torch.ones(batch_size, 1)*33).to(DEVICE)\n",
        "\n",
        "        #initialization\n",
        "        context = values[0,:,:]\n",
        "\n",
        "        attentionPlot = []\n",
        "\n",
        "        for i in range(max_len):\n",
        "            # * Implement Gumble noise and teacher forcing techniques \n",
        "            # * When attention is True, replace values[i,:,:] with the context you get from attention.\n",
        "            # * If you haven't implemented attention yet, then you may want to check the index and break \n",
        "            #   out of the loop so you do not get index out of range errors. \n",
        "\n",
        "            if (isTrain):\n",
        "              #-----------------------Teacher forcing---------------------------\n",
        "              char_embed = teacher_forcing(prediction,i,batch_size,self.embedding,embeddings,\n",
        "                                           teacher_forcing_percent,isTrain = True)\n",
        "              \n",
        "            else:\n",
        "              char_embed = teacher_forcing(prediction,i,batch_size,self.embedding,None,\n",
        "                                           teacher_forcing_percent,isTrain = False)\n",
        "              #-----------------------------------------------------------------\n",
        "          \n",
        "            inp = torch.cat([char_embed, context], dim=1)\n",
        "            hidden_states[0] = self.lstm1(inp, hidden_states[0])\n",
        "\n",
        "            inp_2 = hidden_states[0][0]\n",
        "            hidden_states[1] = self.lstm2(inp_2, hidden_states[1])\n",
        "\n",
        "            ### Compute attention from the output of the second LSTM Cell ###\n",
        "            output = hidden_states[1][0]\n",
        "\n",
        "            #---------------------- Attention-----------------------------------\n",
        "\n",
        "            T,N,value_size = values.shape\n",
        "\n",
        "            context,attention = self.attention(output,key,values,lens)\n",
        "\n",
        "            if batch_idx % 50 == 0 and isTrain:\n",
        "                    currAtten = attention[0].detach().cpu()\n",
        "\n",
        "                    attentionPlot.append(currAtten) \n",
        "\n",
        "            prediction = self.character_prob(torch.cat([output,context], dim=1))\n",
        "            predictions.append(prediction.unsqueeze(1))\n",
        "        #--------------Plotting code------------------------------------------\n",
        "        if batch_idx % 50 == 0 and isTrain:\n",
        "            attentions = torch.stack(attentionPlot, dim=1)\n",
        "\n",
        "            plt.clf()\n",
        "            sns.heatmap(attentions, cmap='GnBu')\n",
        "            plt.savefig(\"./attention/heat_{}s.png\".format(time.time()))\n",
        "        #----------------------------------------------------------------------\n",
        "        return torch.cat(predictions, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4IWtADOctFa"
      },
      "source": [
        "**Seq2Seq**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiIPMVh8cstU"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    '''\n",
        "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
        "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
        "    '''\n",
        "    def __init__(self, input_dim, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=False):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim)\n",
        "        self.decoder = Decoder(vocab_size, hidden_dim, value_size, key_size, isAttended)\n",
        "\n",
        "    def forward(self, speech_input, speech_len, batch_id, text_input=None, isTrain=True, tf_perc = 90):\n",
        "        key, value, lens = self.encoder(speech_input, speech_len)\n",
        "        if (isTrain == True):\n",
        "            predictions = self.decoder(key, value, lens, batch_id, text_input,teacher_forcing_percent = tf_perc)\n",
        "        else:\n",
        "            predictions = self.decoder(key, value, lens, batch_id, text=None, isTrain=False)\n",
        "        return predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1mQOLkIlSdQ"
      },
      "source": [
        "**Train and Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTutwkDaoSj9"
      },
      "source": [
        "def get_mask(lengths):\n",
        "\n",
        "  lengths = torch.tensor(lengths).to(DEVICE)\n",
        "    \n",
        "  max_length = torch.max(lengths)\n",
        "\n",
        "  mask = torch.arange(max_length)\n",
        "  msk = mask.expand((len(lengths),len(mask))).to(DEVICE)\n",
        "  len_exp = lengths.unsqueeze(1).expand((len(lengths),max_length)).int()\n",
        "  \n",
        "  mask = msk < len_exp     \n",
        "  return mask \n",
        "\n",
        "def get_Levenshtein(prediction,label):\n",
        "\n",
        "  dist_total = 0\n",
        "\n",
        "  for i in range(len(prediction)):\n",
        "    preds = prediction[i]\n",
        "    target = label[i]\n",
        "\n",
        "    lev_dis = Levenshtein.distance(preds,target)\n",
        "\n",
        "    dist_total += lev_dis\n",
        " \n",
        "  return dist_total\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnePbtnfixYc"
      },
      "source": [
        "def train(model, train_loader, criterion, optimizer, epoch,tf_perc = 90):\n",
        "    model.train()\n",
        "    model.to(DEVICE)\n",
        "    start = time.time()\n",
        "\n",
        "    runningLoss = 0\n",
        "    tf = tf_perc\n",
        "    factor = 50 \n",
        "    # 1) Iterate through your loader\n",
        "\n",
        "    for id,(x,xlens,y,ylens) in enumerate(train_loader):\n",
        "\n",
        "      with torch.autograd.set_detect_anomaly(False):\n",
        "\n",
        "        # 2) Set the inputs to the device.\n",
        "        x,xlens,y,ylens = x.to(DEVICE), xlens.to(DEVICE), y.to(DEVICE), ylens.to(DEVICE)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 3) Pass your inputs, and length of speech into the model.\n",
        "        preds = model(x,xlens,id,y,isTrain = True,tf_perc = tf)\n",
        "\n",
        "        # 4) Generate a mask based on the lengths of the text to create a masked loss.\n",
        "        mask = get_mask(ylens).to(DEVICE)\n",
        "\n",
        "        # 5) If necessary, reshape your predictions and origianl text input \n",
        "        # 6) Use the criterion to get the loss.\n",
        "        loss = criterion(preds.reshape((-1,preds.shape[2])),y.reshape(-1))\n",
        "\n",
        "        # 7) Use the mask to calculate a masked loss. \n",
        "        masked_loss = torch.sum(loss * mask.reshape(-1))/torch.sum(mask)\n",
        "\n",
        "        Loss = masked_loss.item()\n",
        "        Perplex = torch.exp(masked_loss).item()\n",
        "        runningLoss += Loss\n",
        "        \n",
        "        # 8) Run the backward pass on the masked loss. \n",
        "        masked_loss.backward()\n",
        "\n",
        "        # 9) Use torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
        "\n",
        "        # 10) Take a step with your optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # 11) Normalize the masked loss\n",
        "\n",
        "        # 12) Optionally print the training loss after every N batches        \n",
        "\n",
        "        if(id%50==0):\n",
        "          print(\"Epoch:\",epoch,\"Batch_id:\",id,\"Loss: \", runningLoss/factor,'Perplexity:', Perplex, 'time elapsed: ',(time.time()-start))\n",
        "          factor+=50\n",
        "\n",
        "\n",
        "        del x\n",
        "        del xlens\n",
        "        del y\n",
        "        del ylens\n",
        "        torch.cuda.empty_cache()\n",
        "      end = time.time()\n",
        "\n",
        "def val(model, valid_loader, criterion, optimizer, epoch):\n",
        "    model.eval()\n",
        "    model.to(DEVICE)\n",
        "    start = time.time()\n",
        "  \n",
        "    leven_dist = 0\n",
        "    total_seq_len = 0\n",
        "\n",
        "    for id,(x,xlens,y,ylens) in enumerate(valid_loader):\n",
        "\n",
        "        x,xlens,y,ylens = x.to(DEVICE), xlens.to(DEVICE), y.to(DEVICE), ylens.to(DEVICE)\n",
        "\n",
        "        preds = model(x,xlens,id,y,isTrain = False)\n",
        "\n",
        "        # predn_nums = preds.argmax(-1).detach().cpu().numpy()\n",
        "        predn_nums = preds.argmax(-1).detach().cpu()\n",
        "\n",
        "        predicted_text = transform_index_to_letter(predn_nums,letter2index,index2letter)\n",
        "        true_text = transform_index_to_letter(y.detach().cpu(),letter2index,index2letter)\n",
        "\n",
        "        dist = get_Levenshtein(predicted_text,true_text)\n",
        "\n",
        "        leven_dist+=dist\n",
        "        total_seq_len += len(predicted_text)\n",
        "        if(id%20==0):\n",
        "          print('Levenshtein: ', leven_dist/total_seq_len)\n",
        "          print(\"Predicted text:\",predicted_text[0],\"\\n\\n\",\"True text:\", true_text[0],\"\\n\\n\")\n",
        "\n",
        "\n",
        "        del x\n",
        "        del xlens\n",
        "        del y\n",
        "        del ylens\n",
        "        torch.cuda.empty_cache()\n",
        "    end = time.time()\n",
        "    return leven_dist/total_seq_len\n",
        "\n",
        "def test(model, test_loader):\n",
        "    ### Write your test code here! ###\n",
        "    result = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for id,(x,xlens) in enumerate(test_loader):\n",
        "        x,xlens= x.to(DEVICE), xlens.to(DEVICE)\n",
        "\n",
        "        preds = model(x,xlens,id,None,isTrain = False)\n",
        "\n",
        "        predn_nums = preds.argmax(-1).detach().cpu().numpy()\n",
        "        \n",
        "        predicted_text = transform_index_to_letter(predn_nums,letter2index,index2letter)\n",
        "\n",
        "        result += predicted_text\n",
        "    \n",
        "      idxs = np.array(list(range(len(result))))\n",
        "      predictions = np.array(result)\n",
        "      df = pd.DataFrame({\"id\" : idxs, \"label\" : predictions})\n",
        "      df.to_csv('please_work_TF0.2.csv',index = False)\n",
        "      \n",
        "      return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPIb8Gi4rqEj"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2XVyXEjx1FE"
      },
      "source": [
        "model = Seq2Seq(input_dim=40,vocab_size=len(LETTER_LIST),\n",
        "                hidden_dim = 256, isAttended = True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.75, patience=1, verbose=True, threshold=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGYl4uUZydgI"
      },
      "source": [
        "model.to(DEVICE)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgeqadmR1VPh"
      },
      "source": [
        "tf = 90\n",
        "for epoch in range(60):\n",
        "  if epoch == 27:\n",
        "    tf = 80\n",
        "  if epoch == 50:\n",
        "    tf = 70\n",
        "  startTime = time.time()\n",
        "  train(model, train_loader, criterion, optimizer, epoch,tf_perc=tf)\n",
        "  lev_dist = val(model, valid_loader, criterion, optimizer, epoch)\n",
        "  scheduler.step(lev_dist)\n",
        "  file = \"/content/gdrive/My Drive/18786/HW4/wt_ckpt/0.1_lockwtty_epoch{0}.pth\".format(epoch+1)\n",
        "  torch.save(model.state_dict(),file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPQgJ5l_iqis"
      },
      "source": [
        "state_dict = torch.load('/content/gdrive/My Drive/18786/HW4/wt_ckpt/0.1_lockwtty_epoch44.pth')\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gsDscacaVNa"
      },
      "source": [
        "test_pred = test(model,test_loader) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}